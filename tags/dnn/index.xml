<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dnn on LESS IS MORE</title>
    <link>http://r9y9.github.io/tags/dnn/</link>
    <description>Recent content in Dnn on LESS IS MORE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Tue, 10 Oct 2017 11:45:32 +0900</lastBuildDate>
    
	<atom:link href="http://r9y9.github.io/tags/dnn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GAN 日本語音声合成 [arXiv:1709.08041]</title>
      <link>http://r9y9.github.io/blog/2017/10/10/gantts-jp/</link>
      <pubDate>Tue, 10 Oct 2017 11:45:32 +0900</pubDate>
      
      <guid>http://r9y9.github.io/blog/2017/10/10/gantts-jp/</guid>
      <description>10&amp;frasl;11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: http://ieeexplore.ieee.org/document/8063435/
arXiv論文リンク: arXiv:1709.08041
前回の記事 の続きです。これでこのシリーズは終わりの予定です。
前回は英語音声合成でしたが、以前書いた DNN日本語音声合成の記事 で使ったデータと同じものを使い、日本語音声合成をやってみましたので、結果を残しておきます。
実験 実験条件 HTSのNIT-ATR503のデモデータ (ライセンス) から、wavデータ503発話を用います。442を学習用、56を評価用、残り5をテスト用にします（※英語音声とtrain/evalの比率は同じです）。継続長モデルは、state-levelではなくphone-levelです。サンプリング周波数が48kHzなので、mgcの次元を25から60に増やしました。モデル構造は、すべて英語音声合成の場合と同じです。ADV loss は0次を除くmgcを用いて計算しました。F0は入れていません。
gantts の jpブランチ をチェックアウトして、以下のシェルを実行すると、ここに貼った結果が得られます。
 ./jp_tts_demo.sh jp_tts_order59  ただし、シェル中に、HTS_ROOT という変数があり、シェル実行前に、環境に合わせてディレクトリを指定する必要があります。
diff --git a/jp_tts_demo.sh b/jp_tts_demo.sh index 7a8f12c..b18e604 100755 --- a/jp_tts_demo.sh +++ b/jp_tts_demo.sh @@ -8,7 +8,7 @@ experiment_id=$1 fs=48000 # Needs adjastment -HTS_DEMO_ROOT=~/local/HTS-demo_NIT-ATR503-M001 +HTS_DEMO_ROOT=HTS日本語デモの場所を指定してください # Flags run_duration_training=1  変換音声の比較 音響モデルのみ適用  自然音声 ベースライン GAN  の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。
nitech_jp_atr503_m001_j49
Your browser does not support the audio element.</description>
    </item>
    
    <item>
      <title>【音声合成編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>http://r9y9.github.io/blog/2017/10/09/gantts/</link>
      <pubDate>Mon, 09 Oct 2017 02:00:00 +0900</pubDate>
      
      <guid>http://r9y9.github.io/blog/2017/10/09/gantts/</guid>
      <description>10&amp;frasl;11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: http://ieeexplore.ieee.org/document/8063435/
arXiv論文リンク: arXiv:1709.08041
前回の記事 の続きです。音響モデルの学習にGANを使うというアイデアは、声質変換だけでなく音声合成にも応用できます。CMU ARCTIC を使った英語音声合成の実験を行って、ある程度良い結果がでたので、まとめようと思います。音声サンプルだけ聴きたい方は真ん中の方まで読み飛ばしてください。
 コードはこちら: r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC)  (VCのコードも一緒です) 音声サンプル付きデモノートブックはこちら: The effects of adversarial training in text-to-speech synthesis | nbviewer  前回の記事でも書いた注意書きですが、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。
実験 実験条件 CMU ARCTIC から、話者 slt のwavデータそれぞれ1131発話すべてを用います。 Merlin の slt デモの条件と同様に、1000を学習用、126を評価用、残り5をテスト用にします。継続長モデル（state-level）には Bidirectional-LSTM RNN を、音響モデルには Feed-forward型 のニューラルネットを使用しました1。継続長モデル、音響モデルの両方にGANを取り入れました。論文の肝である ADV loss についてですが、mgcのみ（0次は除く）を使って計算するパターンと、mgc + lf0で計算するパターンとで比較しました2。
実験の結果 (ADV loss: mgcのみ) は、 a5ec247 をチェックアウトして、下記のシェルを実行すると再現できます。</description>
    </item>
    
    <item>
      <title>【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>http://r9y9.github.io/blog/2017/10/05/ganvc/</link>
      <pubDate>Thu, 05 Oct 2017 23:25:36 +0900</pubDate>
      
      <guid>http://r9y9.github.io/blog/2017/10/05/ganvc/</guid>
      <description>10&amp;frasl;11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: http://ieeexplore.ieee.org/document/8063435/
arXiv論文リンク: arXiv:1709.08041
2017年9月末に、表題の 論文 が公開されたのと、nnmnkwii という designed for easy and fast prototyping を目指すライブラリを作ったのもあるので、実装してみました。僕が実験した限りでは、声質変換 (Voice conversion; VC) では安定して良くなりました（音声合成ではまだ実験中です）。この記事では、声質変換について僕が実験した結果をまとめようと思います。音声合成については、また後日まとめます
 コードはこちら: r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC)  (TTSのコードも一緒です) 音声サンプルを聴きたい方はこちら: The effects of adversarial training in voice conversion | nbviewer (※解説はまったくありませんのであしからず)  なお、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。コードに関しては、ここに貼った結果を再現できるように気をつけました。
概要 一言でいえば、音響モデルの学習に Generative Adversarial Net (GAN) を導入する、といったものです。少し具体的には、
 音響モデル（生成モデル）が生成した音響特徴量を偽物か本物かを見分けようとする識別モデルと、 生成誤差を小さくしつつ (Minimum Generation Error loss; MGE loss の最小化) 、生成した特徴量を識別モデルに本物だと誤認識させようとする (Adversarial loss; ADV loss の最小化) 生成モデル  を交互に学習することで、自然音声の特徴量と生成した特徴量の分布を近づけるような、より良い音響モデルを獲得する、といった方法です。</description>
    </item>
    
    <item>
      <title>DNN音声合成のためのライブラリの紹介とDNN日本語音声合成の実装例</title>
      <link>http://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</link>
      <pubDate>Wed, 16 Aug 2017 23:10:56 +0900</pubDate>
      
      <guid>http://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</guid>
      <description>nnmnkwii というDNN音声合成のためのライブラリを公開しましたので、その紹介をします。
https://t.co/p8MnOxkVoH Library to build speech synthesis systems designed for easy and fast prototyping. Open sourced:)
&amp;mdash; 山本りゅういち (@r9y9) August 14, 2017 
ドキュメントの最新版は https://r9y9.github.io/nnmnkwii/latest/ です。以下に、いくつかリンクを貼っておきます。
 なぜ作ったのか、その背景の説明と設計 (日本語) クイックガイド DNN英語音声合成のチュートリアル  よろしければご覧ください1。
ドキュメントは、だいたい英語でお硬い雰囲気で書いたので、この記事では、日本語でカジュアルに背景などを説明しようと思うのと、（ドキュメントには英語音声合成の例しかないので）HTSのデモに同梱のATR503文のデータセットを使って、DNN日本語音声合成 を実装する例を示したいと思います。結果だけ知りたい方は、音声サンプルが下の方にあるので、適当に読み飛ばしてください。
なぜ作ったのか 一番大きな理由は、僕が 対話環境（Jupyter, IPython等） で使えるツールがほしかったからです2。 僕は結構前からREPL (Read-Eval-Print-Loop) 信者で、プログラミングのそれなりの時間をREPLで過ごします。 IDEも好きですし、emacsも好きなのですが、同じくらいJupyterやJuliaのREPLが好きです。 用途に応じて使い分けますが、特に何かデータを分析する必要があるような時に、即座にデータを可視化できるJupyter notebookは、僕にとってプログラミングに欠かせないものになっています。
ところが、HTSの後継として生まれたDNN音声合成ツールである Merlin は、コマンドラインツールとして使われる想定のもので、僕の要望を満たしてくれるものではありませんでした。 とはいえ、Merlinは優秀な音声研究者たちの産物であり、当然役に立つ部分も多く、使っていました。しかし、ことプロトタイピングにおいては、やはり対話環境でやりたいなあという思いが強まっていきました。
新しく作るのではなく、Merlinを使い続ける、Merlinを改善する方針も考えました。僕がMerlinを使い始めた頃、Merlinはpython3で動かなかったので、動くように プルリク を出したこともあるのですが、まぁレビューに数カ月もかかってしまったので、これは新しいものを作った方がいいな、と思うに至りました。
以上が、僕が新しくツール作ろうと思った理由です。
特徴 さて、Merlinに対する敬意と不満から生まれたツールでありますが、その特徴を簡単にまとめます。
 対話環境 での使用を前提に、設計されています。コマンドラインツールはありません。ユーザが必要に応じて作ればよい、という考えです。 DNN音声合成のデモをノートブック形式で提供しています。 大規模データでも扱えるように、データセットとデータセットのイテレーション（フレーム毎、発話毎の両方）のユーティリティが提供されています Merlinとは異なり、音響モデルは提供しません。自分で実装する必要があります（が、今の時代簡単ですよね、lstmでも数行で書けるので 任意の深層学習フレームワークと併せて使えるように、設計されています3（autogradパッケージのみ、今のところPyTorch依存です 言語特徴量の抽出の部分は、Merlinのコードをリファクタして用いています。そのせいもあって、Merlinのデモと同等のパフォーマンスを簡単に実現できます。  対象ユーザ まずはじめに、大雑把にいって、音声合成の研究（or その真似事）をしてみたい人が主な対象です。 自前のデータを元に、ブラックボックスでいいので音声合成エンジンを作りたい、という人には厳しいかもしれません。その前提を元に、少し整理します。</description>
    </item>
    
    <item>
      <title>DNN統計的音声合成ツールキット Merlin の中身を理解をする</title>
      <link>http://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</link>
      <pubDate>Wed, 16 Aug 2017 03:00:00 +0900</pubDate>
      
      <guid>http://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</guid>
      <description>この記事では、音声合成ツールキットであるMerlinが、具体的に何をしているのか（特徴量の正規化、無音区間の削除、ポストフィルタなど、コードを読まないとわからないこと）、その中身を僕が理解した範囲でまとめます。 なお、HMM音声合成について簡単に理解していること（HMMとは、状態とは、フルコンテキストラベルとは、くらい）を前提とします。
はじめに Merlinの概要については以下をご覧ください。
 Wu, Zhizheng, Oliver Watts, and Simon King. &amp;ldquo;Merlin: An open source neural network speech synthesis system.&amp;rdquo; Proc. SSW, Sunnyvale, USA (2016). &amp;ldquo;A Demonstration of the Merlin Open Source Neural Network Speech Synthesis System&amp;rdquo; 公式ドキュメント  Merlinにはデモスクリプトがついています。基本的にユーザが使うインタフェースはrun_merlin.pyというコマンドラインスクリプトで、 デモスクリプトではrun_merlin.pyに用途に応じた設定ファイルを与えることで、継続長モデルの学習/音響モデルの学習/パラメータ生成など、音声合成に必要なステップを実現しています。
デモスクリプトを実行すると、音声データ (wav) と言語特徴量（HTSのフルコンテキストラベル）から、変換音声が合成されるところまでまるっとやってくれるのですが、それだけでは内部で何をやっているのか、理解することはできません。 ツールキットを使う目的が、自分が用意したデータセットで音声合成器を作りたい、といった場合には、特に内部を知る必要はありません。 また、設定ファイルをちょこっといじるだけでこと済むのであれば、知る必要はないかもしれません。 しかし、モデル構造を変えたい、学習アルゴリズムを変えたい、ポストフィルタを入れたい、といったように、少し進んだ使い方をしようとすれば、内部構造を理解しないとできないことも多いと思います。
run_merlin.py はあらゆる処理 (具体的にはあとで述べます) のエントリーポイントになっているがゆえに、コードはなかなかに複雑になっています1。この記事では、run_merlin.pyがいったい何をしているのかを読み解いた結果をまとめます。
Merlinでは提供しないこと Merlinが何を提供してくれるのかを理解する前に、何を提供しないのか、をざっくりと整理します。以下のとおりです。
 Text-processing (Frontend) Speech analysis/synthesis (Backend)  HTSと同様に、frontend, backendといった部分は提供していません。Merlinの論文にもあるように、HTSの影響を受けているようです。
Frontendには、英語ならFestival、BackendにはWORLDやSTRAIGHTを使ってよろしくやってね、というスタンスです。 Backendに関しては、Merlinのインストールガイドにあるように、WOLRDをインストールするように促されます。
デモスクリプトでは、Frontendによって生成されたフルコンテキストラベル（HTS書式）が事前に同梱されているので、Festivalをインストールする必要はありません。 misc以下に、Festivalを使ってフルコンテキストラベルを作るスクリプト (make_labels) があるので、デモデータ以外のデータセットを使う場合は、それを使います。</description>
    </item>
    
  </channel>
</rss>
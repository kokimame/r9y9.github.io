<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: voice-conversion, | LESS IS MORE]]></title>
  <link href="http://r9y9.github.io/blog/categories/voice-conversion/atom.xml" rel="self"/>
  <link href="http://r9y9.github.io/"/>
  <updated>2014-07-21T00:46:19+09:00</updated>
  <id>http://r9y9.github.io/</id>
  <author>
    <name><![CDATA[Ryuichi Yamamoto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[統計的声質変換クッソムズすぎワロタ（実装の話）]]></title>
    <link href="http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/"/>
    <updated>2014-07-13T02:02:00+09:00</updated>
    <id>http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran</id>
    <content type="html"><![CDATA[<h2>まえがき</h2>

<p>前回、<a href="http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/">統計的声質変換クッソムズすぎワロタ &ndash; LESS IS MORE</a> という記事を書いたら研究者の方々等ちょいちょい反応してくださって嬉しかったです。差分スペクトル補正、その道の人が聴いても音質がいいそう。これはいい情報です。</p>

<p>Twitter引用:</p>

<blockquote class="twitter-tweet" lang="en"><p>統計的声質変換クッソムズすぎワロタ - LESS IS MORE <a href="http://t.co/8RkeXIf6Ym">http://t.co/8RkeXIf6Ym</a> <a href="https://twitter.com/r9y9">@r9y9</a>さんから ムズすぎと言いながら，最後の音はしっかり出ているあたり凄いなぁ．</p>&mdash; M. Morise (忍者系研究者) (@m_morise) <a href="https://twitter.com/m_morise/statuses/485339123171852289">July 5, 2014</a></blockquote>


<script async src="http://r9y9.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/ballforest">@ballforest</a> 従来のパラメータ変換と比較すると、音質は従来よりもよさそうな気はしますがスペクトル包絡の性差ががっつりと影響しそうな気もするんですよね。</p>&mdash; 縄文人（妖精系研究者なのです） (@dicekicker) <a href="https://twitter.com/dicekicker/statuses/485380534122463232">July 5, 2014</a></blockquote>


<script async src="http://r9y9.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>異性間に関しては、実験が必要ですね。異性間だとF0が結構変わってくると思いますが、差分スペクトル補正の場合そもそもF0をいじらないという前提なので、F0とスペクトル包絡が完全に独立でない（ですよね？）以上、同姓間に比べて音質は劣化する気はします。簡単にやったところ、少なくとも僕の主観的には劣化しました</p>

<p>ところで、結構いい感じにできたぜひゃっはーと思って、先輩に聞かせてみたら違いわかんねと言われて心が折れそうになりました。やはり現実はつらいです。</p>

<h2>実装の話</h2>

<p>さて、今回は少し実装のことを書こうと思います。学習&amp;変換部分はPythonで書いています。その他はGo（※Goの話は書きません）。</p>

<h2>トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz</h2>

<p>前回トラジェクトリベースは処理が激重だと書きました。なんと、4秒程度の音声（フレームシフト5msで777フレーム）に対して変換部分に600秒ほどかかっていたのですが（重すぎワロタ）、結果から言えばPythonでも12秒くらいまでに高速化されました（混合数64, メルケプの次元数40+デルタ=80次元、分散共分散はfull）。本当にごめんなさい。</p>

<p>何ヶ月か前、ノリでトラジェクトリベースの変換を実装しようと思ってサクッと書いたのがそのままで、つまりとても効率の悪い実装になっていました。具体的には放置していた問題が二つあって、</p>

<ul>
<li>ナイーブな逆行列の計算</li>
<li>スパース性の無視</li>
</ul>


<p>です。特に後者はかなりパフォーマンスに影響していました</p>

<h2>ナイーブな逆行列の計算</h2>

<p><a href="http://d.hatena.ne.jp/sleepy_yoshi/20120513/p1">numpy.linalg.invとnumpy.linalg.solveを用いた逆行列計算 &ndash; 睡眠不足？！ (id:sleepy_yoshi)</a></p>

<p><code>numpy.linalg.inv</code>を使っていましたよね。しかも<code>numpy.linalg.solve</code>のほうが速いことを知っていながら。一ヶ月前の自分を問い詰めたい。<code>numpy.linalg.solve</code>で置き換えたら少し速くなりました。</p>

<ul>
<li>600秒 &ndash;> 570秒 （うろ覚え）</li>
</ul>


<p>1.05倍の高速化（微妙）</p>

<h2>スパース性の無視</h2>

<ul>
<li><a href="http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf">T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007</a>.</li>
</ul>


<p>論文を見ていただければわかるのですが、トラジェクトリベースの変換法における多くの計算は、行列を使って表すことができます。で、論文中の$W$という行列は、サイズがめちゃくちゃでかいのですがほとんどの要素は0です。この性質を使わない理由はないですよね？？</p>

<p>…残念なことに、僕は密行列として扱って計算していました。ほら、疎行列ってちょっと扱いづらいじゃないですか…めんどくさそう…と思って放置してました。ごめんなさい</p>

<p>pythonで疎行列を扱うなら、scipy.sparseを使えば良さそうです。結果、$W$を疎行列として扱うことで行列演算は大きく高速化されました。</p>

<ul>
<li>570秒 &ndash;> 12秒くらい</li>
</ul>


<p>単純に考えると50倍の高速化ですか。本当にアホだった。最初からscipy.sparse使っておけばよかったです。</p>

<p>scipy.sparseの使い方は以下を参考にしました。みなさんぜひ使いましょう</p>

<ul>
<li><a href="http://sucrose.hatenablog.com/entry/2013/04/07/130625">Python で疎行列(SciPy) &ndash; 唯物是真 @Scaled_Wurm</a></li>
<li><a href="http://docs.scipy.org/doc/scipy/reference/sparse.html">Sparse matrices (scipy.sparse) — SciPy v0.14.0 Reference Guide</a></li>
<li><a href="http://lucidfrontier45.wordpress.com/2011/08/02/scipysparse_matmul/">scipy.sparseで疎行列の行列積 | frontier45</a></li>
</ul>


<h2>コード</h2>

<p>メモ的な意味で主要なコードを貼っておきます。</p>

<p>```python</p>

<h1>!/usr/bin/python</h1>

<h1>coding: utf-8</h1>

<p>import numpy as np
from numpy import linalg
from sklearn.mixture import GMM
import scipy.linalg
import scipy.sparse
import scipy.sparse.linalg</p>

<p>class GMMMap:</p>

<pre><code>"""GMM-based frame-by-frame speech parameter mapping. 

GMMMap represents a class to transform spectral features of a source
speaker to that of a target speaker based on Gaussian Mixture Models
of source and target joint spectral features.

Notation
--------
Source speaker's feature: X = {x_t}, 0 &lt;= t &lt; T
Target speaker's feature: Y = {y_t}, 0 &lt;= t &lt; T
where T is the number of time frames.

Parameters
----------
gmm : scipy.mixture.GMM
    Gaussian Mixture Models of source and target joint features

swap : bool
    True: source -&gt; target
    False target -&gt; source

Attributes
----------
num_mixtures : int
    the number of Gaussian mixtures

weights : array, shape (`num_mixtures`)
    weights for each gaussian

src_means : array, shape (`num_mixtures`, `order of spectral feature`)
    means of GMM for a source speaker

tgt_means : array, shape (`num_mixtures`, `order of spectral feature`)
    means of GMM for a target speaker

covarXX : array, shape (`num_mixtures`, `order of spectral feature`, 
    `order of spectral feature`)
    variance matrix of source speaker's spectral feature

covarXY : array, shape (`num_mixtures`, `order of spectral feature`, 
    `order of spectral feature`)
    covariance matrix of source and target speaker's spectral feature

covarYX : array, shape (`num_mixtures`, `order of spectral feature`, 
    `order of spectral feature`)
    covariance matrix of target and source speaker's spectral feature

covarYY : array, shape (`num_mixtures`, `order of spectral feature`, 
    `order of spectral feature`)
    variance matrix of target speaker's spectral feature

D : array, shape (`num_mixtures`, `order of spectral feature`, 
    `order of spectral feature`)
    covariance matrices of target static spectral features

px : scipy.mixture.GMM
    Gaussian Mixture Models of source speaker's features

Reference
---------
  - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
    of Spectral Parameter Trajectory.
    http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf

"""
def __init__(self, gmm, swap=False):
    # D is the order of spectral feature for a speaker
    self.num_mixtures, D = gmm.means_.shape[0], gmm.means_.shape[1]/2
    self.weights = gmm.weights_

    # Split source and target parameters from joint GMM
    self.src_means = gmm.means_[:, 0:D]
    self.tgt_means = gmm.means_[:, D:]
    self.covarXX = gmm.covars_[:, :D, :D]
    self.covarXY = gmm.covars_[:, :D, D:]
    self.covarYX = gmm.covars_[:, D:, :D]
    self.covarYY = gmm.covars_[:, D:, D:]

    # swap src and target parameters
    if swap:
        self.tgt_means, self.src_means = self.src_means, self.tgt_means
        self.covarYY, self.covarXX = self.covarXX, self.covarYY
        self.covarYX, self.covarXY = self.XY, self.covarYX

    # Compute D eq.(12) in [Toda 2007]
    self.D = np.zeros(self.num_mixtures*D*D).reshape(self.num_mixtures, D, D)
    for m in range(self.num_mixtures):
        xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
        self.D[m] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)

    # p(x), which is used to compute posterior prob. for a given source
    # spectral feature in mapping stage.
    self.px = GMM(n_components=self.num_mixtures, covariance_type="full")
    self.px.means_ = self.src_means
    self.px.covars_ = self.covarXX
    self.px.weights_ = self.weights

def convert(self, src):
    """
    Mapping source spectral feature x to target spectral feature y 
    so that minimize the mean least squared error.
    More specifically, it returns the value E(p(y|x)].

    Parameters
    ----------
    src : array, shape (`order of spectral feature`)
        source speaker's spectral feature that will be transformed

    Return
    ------
    converted spectral feature
    """
    D = len(src)

    # Eq.(11)
    E = np.zeros((self.num_mixtures, D))
    for m in range(self.num_mixtures):
        xx = np.linalg.solve(self.covarXX[m], src - self.src_means[m])
        E[m] = self.tgt_means[m] + self.covarYX[m].dot(xx)

    # Eq.(9) p(m|x)
    posterior = self.px.predict_proba(np.atleast_2d(src))

    # Eq.(13) conditinal mean E[p(y|x)]
    return posterior.dot(E)
</code></pre>

<p>class TrajectoryGMMMap(GMMMap):</p>

<pre><code>"""
Trajectory-based speech parameter mapping for voice conversion
based on the maximum likelihood criterion.

Parameters
----------
gmm : scipy.mixture.GMM
    Gaussian Mixture Models of source and target speaker joint features

gv : scipy.mixture.GMM (default=None)
    Gaussian Mixture Models of target speaker's global variance of spectral
    feature

swap : bool (default=False)
    True: source -&gt; target
    False target -&gt; source

Attributes
----------
TODO 

Reference
---------
  - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
    of Spectral Parameter Trajectory.
    http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf
"""
def __init__(self, gmm, T, gv=None, swap=False):
    GMMMap.__init__(self, gmm, swap)

    self.T = T
    # shape[1] = d(src) + d(src_delta) + d(tgt) + d(tgt_delta)
    D = gmm.means_.shape[1] / 4

    ## Setup for Trajectory-based mapping
    self.__construct_weight_matrix(T, D)

    ## Setup for GV post-filtering
    # It is assumed that GV is modeled as a single mixture GMM
    if gv != None:
        self.gv_mean = gv.means_[0]
        self.gv_covar = gv.covars_[0]
        self.Pv = np.linalg.inv(self.gv_covar)

def __construct_weight_matrix(self, T, D):
    # Construct Weight matrix W
    # Eq.(25) ~ (28)
    # TODO(ryuichi) replace self.W to sparse matrix
    self.W = np.zeros((2*D*T, D*T))
    for t in range(T):
        w0 = np.zeros((D, D*T))
        w1 = np.zeros((D, D*T))
        w0[0:,t*D:(t+1)*D] = np.diag(np.ones(D))

        if t-1 &gt;= 0:
            tmp = np.zeros(D)
            tmp.fill(-0.5)
            w1[0:,(t-1)*D:t*D] = np.diag(tmp)
        if t+1 &lt; T:
            tmp = np.zeros(D)
            tmp.fill(0.5)
            w1[0:,(t+1)*D:(t+2)*D] = np.diag(tmp)

        W_t = np.vstack([w0, w1])
        self.W[2*D*t:2*D*(t+1),:] = W_t

    # make self.W as a sparse matrix that is suitable for matrix product
    self.W = scipy.sparse.csr_matrix(self.W)

def convert(self, src):
    """
    Mapping source spectral feature x to target spectral feature y 
    so that maximize the likelihood of y given x.

    Parameters
    ----------
    src : array, shape (`the number of frames`, `the order of spectral feature`)
        a sequence of source speaker's spectral feature that will be
        transformed

    Return
    ------
    a sequence of transformed spectral features
    """
    T, D = src.shape[0], src.shape[1]/2

    if T != self.T:
        self.__construct_weight_matrix(T, D)

    # A suboptimum mixture sequence  (eq.37)
    optimum_mix = self.px.predict(src)

    # Compute E eq.(40)
    self.E = np.zeros((T, 2*D))
    for t in range(T):
        m = optimum_mix[t] # estimated mixture index at time t
        xx = np.linalg.solve(self.covarXX[m], src[t] - self.src_means[m])
        # Eq. (22)
        self.E[t] = self.tgt_means[m] + np.dot(self.covarYX[m], xx)
    self.E = self.E.flatten()

    # Compute D eq.(41). Note that self.D represents D^-1.
    self.D = np.zeros((T, 2*D, 2*D))
    for t in range(T):
        m = optimum_mix[t]
        xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
        # Eq. (23)
        self.D[t] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)
        self.D[t] = np.linalg.inv(self.D[t])
    self.D = scipy.linalg.block_diag(*self.D)

    # represent D as a sparse matrix
    self.D = scipy.sparse.csr_matrix(self.D)

    # Compute target static features
    # eq.(39)
    covar = self.W.T.dot(self.D.dot(self.W))
    y = scipy.sparse.linalg.spsolve(covar, self.W.T.dot(self.D.dot(self.E)),\
                                    use_umfpack=False)
    return y.reshape((T, D))
</code></pre>

<p>```</p>

<h2>結論</h2>

<ul>
<li>疎行列の演算を考えるときは、間違ってもめんどくさいとか思わずに疎行列を積極的に使おう</li>
<li>統計的声質変換ムズすぎ</li>
</ul>


<h2>おまけめも</h2>

<p>僕が変換精度を改善するために考えていることのめも</p>

<ul>
<li>統計的な手法を使う限りover-smoothingの問題はついてくる。ならば、逆にover-smoothingされることで都合の良い特徴量を考えることはできないか</li>
<li>メルケプとかそもそもスペクトル包絡をコンパクトにparamtricに表現するために考えられたもの（だと思ってる）ので、高品質な変換を考えるならばスペクトル包絡をそのまま使うなりした方がいいんじゃないか。とはいえスペクトル包絡をそのまま使うのはぼちぼち高次元なので、個人性に依存する部分を残した形で非線形次元削減したらどうか（例えばニューラルネットを使って統計的に個人性に依存する部分を見つけ出すとか）</li>
<li>time-dependentな関係をモデル化しないとだめじゃないか、確率モデルとして。RNNとか普通に使えそうだし、まぁHMMでもよい</li>
<li>音素境界を推定して、segment単位で変換するのも良いかも</li>
<li>識別モデルもっと使ってもいいんじゃないか</li>
<li>波形合成にSPTKのmlsadfコマンド使ってる？あれ実はフレーム間のメルケプが線形補間されてるんですよね。本当に線形補間でいいんでしょうか？他の補間法も試したらどうですかね</li>
</ul>


<p>こんなかんじですか。おやすみなさい</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[統計的声質変換クッソムズすぎワロタ]]></title>
    <link href="http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/"/>
    <updated>2014-07-05T16:48:00+09:00</updated>
    <id>http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui</id>
    <content type="html"><![CDATA[<p>こんにちは。</p>

<p>最近、統計的声質変換の勉強をしていました。で、メジャーなGMM（混合ガウスモデル）ベースの変換を色々やってみたので、ちょろっと書きます。実は（というほどでもない?）シンプルなGMMベースの方法だと音質クッソ悪くなってしまうんですが、色々試してやっとまともに聞ける音質になったので、試行錯誤の形跡を残しておくとともに、音声サンプルを貼っておきます。ガチ勢の方はゆるりと見守ってください</p>

<p>基本的に、以下の論文を参考にしています</p>

<ul>
<li><a href="http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf">T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007</a>.</li>
</ul>


<h2>GMMベースの声質変換の基本</h2>

<p>シンプルなGMMベースの声質変換は大きく二つのフェーズに分けられます。</p>

<ul>
<li>参照話者と目標話者のスペクトル特徴量の結合GMM $P(x,y)$を学習する</li>
<li>入力$x$が与えらたとき、$P(y|x)$が最大となるようにスペクトル特徴量を変換する</li>
</ul>


<p>あらかじめ話者間の関係をデータから学習しておくことで、未知の入力が来た時にも変換が可能になるわけです。</p>

<p>具体的な変換プロセスとしては、音声を</p>

<ul>
<li>基本周波数</li>
<li>非周期性成分</li>
<li>スペクトル包絡</li>
</ul>


<p>の3つに分解し、スペクトル包絡の部分（≒声質を表す特徴量）に対して変換を行い、最後に波形を再合成するといった方法がよく用いられます。基本周波数や非周期性成分も変換することがありますが、ここではとりあえず扱いません</p>

<p>シンプルな方法では、フレームごとに独立に変換を行います。</p>

<p>GMMベースのポイントは、東大の齋藤先生の以下のツイートを引用しておきます。</p>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/shurabaP">@shurabaP</a> GMMベースの声質変換の肝は、入力xが与えられた時の出力yの条件付き確率P(y|x) が最大になるようにyを選ぶという確率的な考えです。私のショボい自作スクリプトですが、HTKを使ったGMMの学習レシピは研究室内部用に作ってあるので、もし必要なら公開しますよ。</p>&mdash; Daisuke Saito (@dsk_saito) <a href="https://twitter.com/dsk_saito/statuses/48442052534472706">March 17, 2011</a></blockquote>


<script async src="http://r9y9.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>ちなみに僕はscipy.mixture.GMMを使いました。HTKヤダー</p>

<h2>やってみる</h2>

<p>さて、実際にやってみます。データベースには、<a href="ht%0Atp://www.festvox.org/cmu_arctic/">CMU_ARCTIC speech synthesis databases</a>を使います。今回は、女性話者の二人を使いました。</p>

<p>音声の分析合成には、<a href="http://ml.cs.yamanashi.ac.jp/world/">WORLD</a>を使います。WORLDで求めたスペクトル包絡からメルケプストラム（今回は32次元）に変換したものを特徴量として使いました。</p>

<p>学習では、学習サンプル10641フレーム（23フレーズ）、GMMの混合数64、full-covarianceで学習しました。</p>

<h3>変換元となる話者（参照話者）</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362625&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<h3>変換対象となる話者（目標話者）</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362613&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<h3>GMMベースのframe-by-frameな声質変換の結果</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371966&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<p>はぁー、正直聞けたもんじゃないですね。声質は目標話者に近づいている感がありますが、何分音質が悪い。学習条件を色々変えて試行錯誤しましたけどダメでした</p>

<h2>GMMベースの声質変換の弱点</h2>

<p>さて、なぜダメかを考えます。もう考えつくされてる感あるけど、大事なところだけ整理します</p>

<h3>フレーム毎に独立な変換処理</h3>

<p>まず、音声が時間的に独立なわけないですよね。フレームごとに独立に変換すると、時間的に不連続な点が出てきてしまいます。その結果、ちょっとノイジーな音声になってしまったのではないかと考えられます。</p>

<p>これに対する解決法としては、戸田先生の論文にあるように、動的特徴量も併せてGMMを学習して、系列全体の確率が最大となるように変換を考えるトラジェクトリベースのパラメータ生成方法があります。<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>さて、やってみます。参照音声、目標音声は↑で使ったサンプルと同じです。</p>

<h3>トラジェクトリベースの声質変換の結果</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371969&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<p>あんま変わらないですね。計算量めっちゃ食うのに、本当につらい。実装が間違ってる可能性もあるけど…</p>

<p>他の方法を考えるとするならば、まぁいっぱいあると思うんですが、スペクトル包絡なんて時間的に不連続にコロコロ変わるようなもんでもない気がするので、確率モデルとしてそういう依存関係を考慮した声質変換があってもいいもんですけどね。あんま見てない気がします。</p>

<p>ちょっと調べたら見つかったもの↓</p>

<ul>
<li><a href="http://koasas.kaist.ac.kr/bitstream/10203/17632/1/25.pdf">Kim, E.K., Lee, S., Oh, Y.-H. (1997). &ldquo;Hidden Markov Model Based Voice Conversion Using Dynamic Characteristics of Speaker&rdquo;, Proc. of Eurospeech’97, Rhodes, Greece, pp. 2519-2522.</a></li>
</ul>


<h3>過剰な平滑化</h3>

<p>これはGMMに限った話ではないですが、GMMベースのFrame-by-Frameな声質変換の場合でいえば、変換後の特徴量は条件付き期待値を取ることになるので、まぁ常識的に考えて平滑化されますよね。</p>

<p>これに対する解法としては、GV（Global Variance）を考慮する方法があります。これは戸田先生が提案されたものですね。</p>

<p>さて、やってみます。wktk</p>

<h3>GVを考慮したトラジェクトリベースの声質変換の結果</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371971&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<p>多少ましになった気もしなくもないけど、やっぱり音質はいまいちですね。そして計算量は激マシします。本当につらい。学会で聞いたGVありの音声はもっと改善してた気がするんだけどなー音声合成の話だけど。僕の実装が間違ってるんですかね…</p>

<h2>ムズすぎわろた</h2>

<p>以上、いくつか試しましたが、統計的声質変換は激ムズだということがわかりました。え、ここで終わるの？という感じですが、最後に一つ別の手法を紹介します。</p>

<h2>差分スペクトル補正に基づく統計的声質変換</h2>

<p>これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。</p>

<p>詳細は、以下の予稿をどうぞ</p>

<p><a href="http://www.phontron.com/paper/kobayashi14asj.pdf">小林 和弘, 戸田 智基, Graham Neubig, Sakriani Sakti, 中村 哲. &ldquo;差分スペクトル補正に基づく統計的歌声声質変換&rdquo;, 日本音響学会2014年春季研究発表会(ASJ). 東京. 2014年3月.</a></p>

<p>では、やってみます。歌声ではなく話し声ですが。他の声質変換の結果とも聴き比べてみてください。</p>

<h3>差分スペクトル補正に基づく声質変換の結果</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362603&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>


<p>かなり音声の自然性は上がりましたね。これはヘタすると騙されるレベル。本当に素晴らしいです。しかも簡単にできるので、お勧めです。↑のは、GMMに基づくframe-by-frameな変換です。計算量も軽いので、リアルタイムでもいけますね。</p>

<h2>おわりに</h2>

<p>声質変換であれこれ試行錯誤して、ようやくスタートラインにたてた感があります。今後は新しい方法を考えようかなーと思ってます。</p>

<p>おわり</p>

<h2>おわび</h2>

<blockquote class="twitter-tweet" lang="en"><p>お盆の間に学習ベースの声質変換のプログラム書く（宿題） <a href="https://twitter.com/hashtag/%E5%AE%A3%E8%A8%80?src=hash">#宣言</a></p>&mdash; 山本りゅういち (@r9y9) <a href="https://twitter.com/r9y9/statuses/366928228465655808">August 12, 2013</a></blockquote>


<script async src="http://r9y9.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>約1年かかりました……。本当に申し訳ありませんでした(´･_･`)</p>

<h2>追記</h2>

<p>Twitterで教えてもらいました。トラジェクトリベースで学習も変換も行う研究もありました</p>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/r9y9">@r9y9</a> つ トラジェクトリＧＭＭな特徴量変換 <a href="http://t.co/kUn7bp9EUt">http://t.co/kUn7bp9EUt</a></p>&mdash; 縄文人（妖精系研究者なのです） (@dicekicker) <a href="https://twitter.com/dicekicker/statuses/485376823308455936">July 5, 2014</a></blockquote>


<script async src="http://r9y9.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>ただ、これはトラジェクトリベースのパラメータ生成法の提案であって、トラジェクトリモデル自体を学習してるわけではないんだよなー。普通に考えると学習もトラジェクトリで考える方法があっていい気がするが、 <del>まだ見てないですね。</del> ありました。追記参照<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
</feed>

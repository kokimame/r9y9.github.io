<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python, | LESS IS MORE]]></title>
  <link href="http://r9y9.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://r9y9.github.io/"/>
  <updated>2014-07-29T22:35:28+09:00</updated>
  <id>http://r9y9.github.io/</id>
  <author>
    <name><![CDATA[Ryuichi Yamamoto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[PythonによるニューラルネットのToyコード]]></title>
    <link href="http://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/"/>
    <updated>2014-05-11T01:20:00+09:00</updated>
    <id>http://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code</id>
    <content type="html"><![CDATA[<p>1000番煎じだけど、知り合いにニューラルネットを教えていて、その過程で書いたコード。わかりやすさ重視。</p>

<p>このために、誤差伝播法をn回導出しました（意訳：何回もメモなくしました）</p>

<p>```python</p>

<h1>!/usr/bin/python</h1>

<h1>coding: utf-8</h1>

<h1>ニューラルネットワーク(Feed-Forward Neural Networks)の学習、認識の</h1>

<h1>デモコードです。</h1>

<h1>誤差伝搬法によってニューラルネットを学習します。</h1>

<h1>XORの学習、テストの簡単なデモコードもついています</h1>

<h1>2014/05/10 Ryuichi Yamamoto</h1>

<p>import numpy as np</p>

<p>def sigmoid(x):</p>

<pre><code>return 1.0 / (1.0 + np.exp(-x))
</code></pre>

<p>def dsigmoid(y):</p>

<pre><code>return y * (1.0 - y)
</code></pre>

<p>class NeuralNet:</p>

<pre><code>def __init__(self, num_input, num_hidden, num_output):
    """
    パラメータの初期化
    """
    # 入力層から隠れ層への重み行列
    self.W1 = np.random.uniform(-1.0, 1.0, (num_input, num_hidden))
    self.hidden_bias = np.ones(num_hidden, dtype=float)
    # 隠れ層から出力層への重み行列
    self.W2 = np.random.uniform(-1.0, 1.0, (num_hidden, num_output))
    self.output_bias = np.ones(num_output, dtype=float)

def forward(self, x):
    """
    前向き伝搬の計算
    """
    h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
    return sigmoid(np.dot(self.W2.T, h) + self.output_bias)

def cost(self, data, target):
    """
    最小化したい誤差関数
    """
    N = data.shape[0]
    E = 0.0
    for i in range(N):
        y, t = self.forward(data[i]), target[i]
        E += np.sum((y - t) * (y - t))
    return 0.5 * E / float(N)

def train(self, data, target, epoches=30000, learning_rate=0.1,\
          monitor_period=None):
    """
    Stochastic Gradient Decent (SGD) による学習
    """
    for epoch in range(epoches):
        # 学習データから1サンプルをランダムに選ぶ
        index = np.random.randint(0, data.shape[0])
        x, t = data[index], target[index]

        # 入力から出力まで前向きに信号を伝搬
        h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
        y = sigmoid(np.dot(self.W2.T, h) + self.output_bias)

        # 隠れ層-&gt;出力層の重みの修正量を計算
        output_delta = (y - t) * dsigmoid(y)
        grad_W2 = np.dot(np.atleast_2d(h).T, np.atleast_2d(output_delta))

        # 隠れ層-&gt;出力層の重みを更新
        self.W2 -= learning_rate * grad_W2
        self.output_bias -= learning_rate * output_delta

        # 入力層-&gt;隠れ層の重みの修正量を計算
        hidden_delta = np.dot(self.W2, output_delta) * dsigmoid(h)
        grad_W1 = np.dot(np.atleast_2d(x).T, np.atleast_2d(hidden_delta))

        # 入力層-&gt;隠れ層の重みを更新
        self.W1 -= learning_rate * grad_W1
        self.hidden_bias -= learning_rate * hidden_delta

        # 現在の目的関数の値を出力
        if monitor_period != None and epoch % monitor_period == 0:
            print "Epoch %d, Cost %f" % (epoch, self.cost(data, target))

    print "Training finished."

def predict(self, x):
    """
    出力層の最も反応するニューロンの番号を返します
    """
    return np.argmax(self.forward(x))
</code></pre>

<p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser(description="Specify options")
parser.add_argument("--epoches", dest="epoches", type=int, required=True)
parser.add_argument("--learning_rate", dest="learning_rate",\
                    type=float, default=0.1)
parser.add_argument("--hidden", dest="hidden", type=int, default=20)
args = parser.parse_args()

nn = NeuralNet(2, args.hidden, 1)

data = np.array([[0, 0], [0 ,1], [1, 0], [1, 1]])
target = np.array([0, 1, 1, 0])

nn.train(data, target, args.epoches, args.learning_rate,\
         monitor_period=1000)

for x in data:
    print "%s : predicted %s" % (x, nn.forward(x))
</code></pre>

<p>```</p>

<p>```python</p>

<h1>!/usr/bin/python</h1>

<h1>coding: utf-8</h1>

<h1>MNISTを用いたニューラルネットによる手書き数字認識のデモコードです</h1>

<h1>学習方法やパラメータによりますが、だいたい 90 ~ 97% くらいの精度出ます。</h1>

<h1>使い方は、コードを読むか、</h1>

<h1>python mnist_net.py -h</h1>

<h1>としてください</h1>

<h1>参考までに、</h1>

<h1>python mnist_net.py &mdash;epoches 50000 &mdash;learning_rate 0.1 &mdash;hidden 100</h1>

<h1>とすると、テストセットに対して、93.2%の正解率です</h1>

<h1>僕の環境では、学習、認識合わせて（だいたい）5分くらいかかりました。</h1>

<h1>2014/05/10 Ryuichi Yamamoto</h1>

<p>import numpy as np
from sklearn.externals import joblib
import cPickle
import gzip
import os</p>

<h1>作成したニューラルネットのパッケージ</h1>

<p>import net</p>

<p>def load_mnist_dataset(dataset):</p>

<pre><code>"""
MNISTのデータセットをダウンロードします
"""
# Download the MNIST dataset if it is not present
data_dir, data_file = os.path.split(dataset)
if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':
    import urllib
    origin = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'
    print 'Downloading data from %s' % origin
    urllib.urlretrieve(origin, dataset)

f = gzip.open(dataset, 'rb')
train_set, valid_set, test_set = cPickle.load(f)
f.close()

return train_set, valid_set, test_set
</code></pre>

<p>def augument_labels(labels, order):</p>

<pre><code>"""
1次元のラベルデータを、ラベルの種類数(order)次元に拡張します
"""
new_labels = []
for i in range(labels.shape[0]):
    v = np.zeros(order)
    v[labels[i]] = 1
    new_labels.append(v)

return np.array(new_labels).reshape((labels.shape[0], order))        
</code></pre>

<p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser(description="MNIST手書き数字認識のデモ")
parser.add_argument("--epoches", dest="epoches", type=int, required=True)
parser.add_argument("--learning_rate", dest="learning_rate",\
                    type=float, default=0.1)
parser.add_argument("--hidden", dest="hidden", type=int, default=100)
args = parser.parse_args()

train_set, valid_set, test_set = load_mnist_dataset("mnist.pkl.gz")
n_labels = 10 # 0,1,2,3,4,5,6,7,9
n_features = 28*28

# モデルを新しく作る
nn = net.NeuralNet(n_features, args.hidden, n_labels)

# モデルを読み込む
# nn = joblib.load("./nn_mnist.pkl")

nn.train(train_set[0], augument_labels(train_set[1], n_labels),\
         args.epoches, args.learning_rate, monitor_period=2000)

## テスト
test_data, labels = test_set
results = np.arange(len(test_data), dtype=np.int)
for n in range(len(test_data)):
    results[n] = nn.predict(test_data[n])
    # print "%d : predicted %s, expected %s" % (n, results[n], labels[n])
print "recognition rate: ", (results == labels).mean()

# モデルを保存
model_filename = "nn_mnist.pkl"
joblib.dump(nn, model_filename, compress=9)
print "The model parameters are dumped to " + model_filename
</code></pre>

<p>```</p>

<p><a href="https://github.com/r9y9/python-neural-net-toy-codes">https://github.com/r9y9/python-neural-net-toy-codes</a></p>

<p>以下のようなコマンドを叩いて、正解率が97%くらいになるまで学習してから入力層から隠れ層への重みを可視化してみた</p>

<p>```bash</p>

<h1>python mnist_net.py &mdash;epoches 50000 &mdash;learning_rate 0.1 &mdash;hidden 100 # epochesは適当に</h1>

<p>```</p>

<p><img class="center" src="/images/nn_mnist_W1_100.png" title="&ldquo;Input to Hidden weight filters after traingned on MNIST.&rdquo;" ></p>

<p>興味深いことに、RBMと違って重み行列の解釈はしにくい。生成モデルの尤度を最大化することと、誤差を最小化することはこんなにも違うんだなぁというこなみな感想</p>

<p>RBMについては、以下へ</p>

<p><a href="http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/">Restricted Boltzmann Machines with MNIST &ndash; LESS IS MORE</a></p>

<p>おわり</p>
]]></content>
  </entry>
  
</feed>
